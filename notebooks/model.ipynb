{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import seaborn_image as isns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comnist_data = np.loadtxt('./../datasets/latin_data.csv', delimiter=\",\", dtype=\"float32\")\n",
    "comnist_label = np.loadtxt('./../datasets/latin_label.csv', delimiter=\",\", dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying letter G\n",
      "No. of Obs. : 784\n",
      "Min. Value : 0.0\n",
      "Max. Value : 1.0\n",
      "Mean : 0.08124786615371704\n",
      "Variance : 0.06019669398665428\n",
      "Skewness : 2.953777313232422\n"
     ]
    },
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "i = np.random.randint(comnist_data.shape[0])\n",
    "my_letter = np.flip(comnist_data[i].reshape(28, 28), 0)\n",
    "my_label = string.ascii_uppercase[int(comnist_label[i])]\n",
    "print(f\"Displaying letter {my_label}\")\n",
    "isns.imgplot(my_letter, cmap=\"gray\", describe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "\n",
    "\n",
    "class Ð¡oMNISTDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        labels_dir,\n",
    "        data_dir,\n",
    "        transform=None,\n",
    "        target_transform=None\n",
    "    ):\n",
    "        self.img_labels = pd.read_csv(labels_dir)\n",
    "        self.img_features = pd.read_csv(data_dir)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.asarray(self.img_features.iloc[idx], dtype=np.float32).reshape(28,28)\n",
    "        label = self.img_labels.iloc[idx, 0]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training_samples: 10897\n",
      "validation_samples: 1924\n",
      "\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "comnnist_dataset = Ð¡oMNISTDataset(\n",
    "    labels_dir='./../datasets/latin_label.csv',\n",
    "    data_dir='./../datasets/latin_data.csv',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((64,64)),\n",
    "        transforms.functional.vflip,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=0.0942,\n",
    "                             std=0.2352)\n",
    "    ])\n",
    ")\n",
    "# TODO: Refactor to const file etc\n",
    "TRAINING_PERCENT = 0.85\n",
    "training_samples = int(len(comnnist_dataset) * TRAINING_PERCENT)\n",
    "validation_samples = len(comnnist_dataset) - training_samples\n",
    "print(f'''\n",
    "training_samples: {training_samples}\n",
    "validation_samples: {validation_samples}\n",
    "''')\n",
    "\n",
    "isns.imshow(comnnist_dataset[100][0].squeeze(0))\n",
    "print(comnnist_dataset[100][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image = comnnist_dataset[100][0]\n",
    "plt.imshow(image.permute(1, 2 ,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imgs = torch.stack([img_t for img_t, _ in comnnist_dataset], dim=3)\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imgs.view(1, -1).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imgs.view(1, -1).std(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(\n",
    "    comnnist_dataset,\n",
    "    [training_samples, validation_samples],\n",
    "    generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: unsure to refactor batch_size to be in separate variable\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_i = iter(train_loader)\n",
    "images, labels = data_i.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20 / 2, idx + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(string.ascii_uppercase[int(labels[idx].item())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "width, height = img.shape\n",
    "thresh = img.max() / 2.5\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        val = round(img[x][y], 2) if img[x][y] != 0 else 0\n",
    "        ax.annotate(str(val), xy=(y, x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    color='white' if img[x][y] < thresh else 'black')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_out=26):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 26, dropout: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img, _ = comnnist_dataset[0]\n",
    "img_batch = img.unsqueeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "isns.imshow(img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 26])"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet = AlexNet()\n",
    "out = alexnet(img.unsqueeze(0))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 26])"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = reducedLatinNet(torch.tensor([[0.3,0.4,0.5]]))\n",
    "out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "out = model(img.unsqueeze(0))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Specify loss and optimization functions\n",
    "\n",
    "# specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.Adam(alexnet.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "alexnet.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_train = 0.0\n",
    "    loss_val = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        outputs = alexnet(images)\n",
    "        train_loss = loss_fn(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += train_loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = alexnet(images)\n",
    "            val_loss = loss_fn(outputs, labels)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "            loss_val += val_loss.item()\n",
    "\n",
    "    \n",
    "    print(f'{datetime.datetime.now()} Epoch {epoch}, Training loss {loss_train / len(train_loader)}' +\n",
    "          f', Validation loss {loss_val / len(val_loader)}, Accuracy {correct / total}')\n",
    "    \n",
    "    if loss_val / len(val_loader) <= 0.3 or loss_train / len(train_loader) <=0.15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(dict(epoch=epoch_list, loss=train_loss_list))\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "fig = sns.relplot(\n",
    "    x='epoch'\n",
    "    , y='loss'\n",
    "    , kind='scatter'\n",
    "    , data=data\n",
    ")\n",
    "fig.savefig(\"output_3_80_relu.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "# obtain one batch of test images\n",
    "loader = torch.utils.data.DataLoader(val_set, batch_size=20, shuffle=True)\n",
    "dataiter = iter(loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# get sample outputs\n",
    "output = alexnet(images)\n",
    "# convert output probabilities to predicted class\n",
    "_, preds = torch.max(output, 1)\n",
    "# prep images for display\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20 / 2, idx + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(\n",
    "        \"{} ({})\".format(string.ascii_uppercase[int(preds[idx].item())],string.ascii_uppercase[int(labels[idx].item())]),\n",
    "        color=(\"green\" if preds[idx] == labels[idx] else \"red\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# torch.zeros(1).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval()  # prep model for *evaluation*\n",
    "\n",
    "for data, target in test_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss\n",
    "    test_loss += loss.item() * data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(100):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)')\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dimensionality Reduction using Supervised UMAP\n",
    "Using UMAP implementation from sci-kit learn library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from umap import UMAP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "reducer = UMAP(\n",
    "    n_neighbors=100,\n",
    "    n_components=3,\n",
    "    n_epochs=1000,\n",
    "    min_dist=0.5,\n",
    "    local_connectivity=10,\n",
    "    random_state=42\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_res: (10897, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train_res = 0\n",
    "X_test_res = 0\n",
    "batch = 0\n",
    "\n",
    "reducer_train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=True)\n",
    "reducer_val_loader = torch.utils.data.DataLoader(val_set, batch_size=len(val_set), shuffle=False)\n",
    "\n",
    "X_train_res = np.array()\n",
    "X_test_res = np.array()\n",
    "\n",
    "for images, labels in reducer_train_loader:\n",
    "    # TODO: Notate batch size trick\n",
    "    batch_size = images.shape[0]\n",
    "    images_a = images.reshape(batch_size,-1).numpy()\n",
    "    labels_a = labels.numpy()\n",
    "    X_train_res = reducer.fit_transform(images_a, labels_a)\n",
    "\n",
    "    break\n",
    "\n",
    "\n",
    "print(f'''Shape of X_train_res: {X_train_res.shape}''')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def chart(X, y):\n",
    "    #--------------------------------------------------------------------------#\n",
    "    # This section is not mandatory as its purpose is to sort the data by label\n",
    "    # so, we can maintain consistent colors for digits across multiple graphs\n",
    "\n",
    "    # Concatenate X and y arrays\n",
    "    arr_concat=np.concatenate((X, y.reshape(y.shape[0],1)), axis=1)\n",
    "    # Create a Pandas dataframe using the above array\n",
    "    df=pd.DataFrame(arr_concat, columns=['x', 'y', 'z', 'label'])\n",
    "    # Convert label data type from float to integer\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    # Finally, sort the dataframe by label\n",
    "    df.sort_values(by='label', axis=0, ascending=True, inplace=True)\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    # Create a 3D graph\n",
    "    fig = px.scatter_3d(df, x='x', y='y', z='z', color=df['label'].astype(str), height=900, width=950)\n",
    "\n",
    "    # Update chart looks\n",
    "    fig.update_layout(title_text='UMAP',\n",
    "                      showlegend=True,\n",
    "                      legend=dict(orientation=\"h\", yanchor=\"top\", y=0, xanchor=\"center\", x=0.5),\n",
    "                      scene_camera=dict(up=dict(x=0, y=0, z=1),\n",
    "                                            center=dict(x=0, y=0, z=-0.1),\n",
    "                                            eye=dict(x=1.5, y=-1.4, z=0.5)),\n",
    "                                            margin=dict(l=0, r=0, b=0, t=0),\n",
    "                      scene = dict(xaxis=dict(backgroundcolor='white',\n",
    "                                              color='black',\n",
    "                                              gridcolor='#f0f0f0',\n",
    "                                              title_font=dict(size=10),\n",
    "                                              tickfont=dict(size=10),\n",
    "                                             ),\n",
    "                                   yaxis=dict(backgroundcolor='white',\n",
    "                                              color='black',\n",
    "                                              gridcolor='#f0f0f0',\n",
    "                                              title_font=dict(size=10),\n",
    "                                              tickfont=dict(size=10),\n",
    "                                              ),\n",
    "                                   zaxis=dict(backgroundcolor='lightgrey',\n",
    "                                              color='black',\n",
    "                                              gridcolor='#f0f0f0',\n",
    "                                              title_font=dict(size=10),\n",
    "                                              tickfont=dict(size=10),\n",
    "                                             )))\n",
    "    # Update marker size\n",
    "    fig.update_traces(marker=dict(size=3, line=dict(color='black', width=0.1)))\n",
    "\n",
    "    fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for images, labels in reducer_val_loader:\n",
    "    batch_size = images.shape[0]\n",
    "    images_a = images.reshape(batch_size, -1).numpy()\n",
    "    X_test_res = reducer.transform(images_a)\n",
    "    chart(X_test_res, labels)\n",
    "\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Artificial Neural Network Model\n",
    "That can distinct our reduced dataset by UMAP, from 3 input features to 26 classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [],
   "source": [
    "class ReducedLatinNet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 26) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3, 64)\n",
    "        self.fc2 = nn.Linear(64, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 256)\n",
    "        self.fc5 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(torch.tanh(x))\n",
    "        out = self.fc2(torch.tanh(out))\n",
    "        out = self.fc3(torch.tanh(out))\n",
    "        out = self.fc4(torch.tanh(out))\n",
    "        out = self.fc5(torch.tanh(out))\n",
    "        return out\n",
    "\n",
    "reducedLatinNet = ReducedLatinNet()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "outputs": [
    {
     "data": {
      "text/plain": "ReducedLatinNet(\n  (fc1): Linear(in_features=3, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=256, bias=True)\n  (fc3): Linear(in_features=256, out_features=256, bias=True)\n  (fc4): Linear(in_features=256, out_features=256, bias=True)\n  (fc5): Linear(in_features=256, out_features=26, bias=True)\n)"
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducedLatinNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "(12822, 784)"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comnist_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([12822, 3]), (12822,))"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_images_a = np.load('./../datasets/reduced/latin_data.npy')\n",
    "reduced_images = torch.as_tensor(reduced_images_a)\n",
    "reduced_images.shape, comnist_label.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "reducedCoMNISTDataset = TensorDataset(\n",
    "    reduced_images,\n",
    "    torch.as_tensor(comnist_label)\n",
    "    #torch.nn.functional.one_hot(\n",
    "    #    torch.as_tensor(comnist_label).long(),\n",
    "    #    num_classes=26\n",
    "    #)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "TRAINING_PERCENT = 0.85\n",
    "training_samples = int(len(reducedCoMNISTDataset) * TRAINING_PERCENT)\n",
    "validation_samples = len(reducedCoMNISTDataset) - training_samples\n",
    "\n",
    "reduced_train_set, reduced_val_set = torch.utils.data.random_split(\n",
    "    reducedCoMNISTDataset,\n",
    "    [training_samples, validation_samples],\n",
    "    generator=torch.Generator().manual_seed(42))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [],
   "source": [
    "reduced_train_loader = torch.utils.data.DataLoader(reduced_train_set, batch_size=64, shuffle=True)\n",
    "reduced_val_loader = torch.utils.data.DataLoader(reduced_val_set, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([256, 1, 64, 64]), torch.Size([256]))"
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = iter(train_loader).next()\n",
    "\n",
    "features.shape, labels.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 3]), torch.Size([1]))"
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = iter(reduced_train_loader).next()\n",
    "\n",
    "features.shape, labels.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([256, 1, 3])"
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.unsqueeze(1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â', printEnd = \"\\r\"):\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total:\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Epoch: 0\n",
      "        - Time:           2021-11-27 14:48:15.226098\n",
      "        - Train loss:     3.2503021125904996\n",
      "        - Validation loss 3.2441007783336024\n",
      "        - Accuracy        0.06912681912681913\n",
      "        \n",
      "\n",
      "        Epoch: 1\n",
      "        - Time:           2021-11-27 14:48:16.102691\n",
      "        - Train loss:     3.239471435546875\n",
      "        - Validation loss 3.2331360463173158\n",
      "        - Accuracy        0.13201663201663202\n",
      "        \n",
      "\n",
      "        Epoch: 2\n",
      "        - Time:           2021-11-27 14:48:16.944700\n",
      "        - Train loss:     3.22831840682448\n",
      "        - Validation loss 3.2219662204865487\n",
      "        - Accuracy        0.13773388773388773\n",
      "        \n",
      "\n",
      "        Epoch: 3\n",
      "        - Time:           2021-11-27 14:48:17.776821\n",
      "        - Train loss:     3.2169047015452246\n",
      "        - Validation loss 3.2103844304238596\n",
      "        - Accuracy        0.14033264033264034\n",
      "        \n",
      "\n",
      "        Epoch: 4\n",
      "        - Time:           2021-11-27 14:48:18.994121\n",
      "        - Train loss:     3.2050546874776917\n",
      "        - Validation loss 3.1983013229985393\n",
      "        - Accuracy        0.17827442827442827\n",
      "        \n",
      "\n",
      "        Epoch: 5\n",
      "        - Time:           2021-11-27 14:48:20.161202\n",
      "        - Train loss:     3.192662417540076\n",
      "        - Validation loss 3.1855297396259923\n",
      "        - Accuracy        0.19230769230769232\n",
      "        \n",
      "\n",
      "        Epoch: 6\n",
      "        - Time:           2021-11-27 14:48:21.019457\n",
      "        - Train loss:     3.179287330448976\n",
      "        - Validation loss 3.1718679012790805\n",
      "        - Accuracy        0.19698544698544698\n",
      "        \n",
      "\n",
      "        Epoch: 7\n",
      "        - Time:           2021-11-27 14:48:21.865092\n",
      "        - Train loss:     3.165192210883425\n",
      "        - Validation loss 3.157155236890239\n",
      "        - Accuracy        0.1897089397089397\n",
      "        \n",
      "\n",
      "        Epoch: 8\n",
      "        - Time:           2021-11-27 14:48:22.704240\n",
      "        - Train loss:     3.1500323962049874\n",
      "        - Validation loss 3.141212455687984\n",
      "        - Accuracy        0.18607068607068608\n",
      "        \n",
      "\n",
      "        Epoch: 9\n",
      "        - Time:           2021-11-27 14:48:23.547868\n",
      "        - Train loss:     3.1332891112879704\n",
      "        - Validation loss 3.123838832301478\n",
      "        - Accuracy        0.18711018711018712\n",
      "        \n",
      "Epoch: 499 Accuracy: 0.36: |ââââ----------------------------------------------| 10.0% N of Epochs: 5000\r\n",
      "        Epoch: 500\n",
      "        - Time:           2021-11-27 14:55:01.269335\n",
      "        - Train loss:     1.8773307333215636\n",
      "        - Validation loss 1.8821003052496141\n",
      "        - Accuracy        0.36538461538461536\n",
      "        \n",
      "Epoch: 999 Accuracy: 0.39: |âââââââââ-----------------------------------------| 20.0% N of Epochs: 5000\r\n",
      "        Epoch: 1000\n",
      "        - Time:           2021-11-27 15:02:13.967258\n",
      "        - Train loss:     1.7097473493096425\n",
      "        - Validation loss 1.7518527007872058\n",
      "        - Accuracy        0.3814968814968815\n",
      "        \n",
      "Epoch: 1499 Accuracy: 0.48: |ââââââââââââââ------------------------------------| 30.0% N of Epochs: 5000\r\n",
      "        Epoch: 1500\n",
      "        - Time:           2021-11-27 15:09:43.548180\n",
      "        - Train loss:     1.6133124605256912\n",
      "        - Validation loss 1.754046424742668\n",
      "        - Accuracy        0.3918918918918919\n",
      "        \n",
      "Epoch: 1999 Accuracy: 0.47: |âââââââââââââââââââ-------------------------------| 40.0% N of Epochs: 5000\r\n",
      "        Epoch: 2000\n",
      "        - Time:           2021-11-27 15:17:32.536774\n",
      "        - Train loss:     1.5348721882056076\n",
      "        - Validation loss 1.5616169245012346\n",
      "        - Accuracy        0.49324324324324326\n",
      "        \n",
      "Epoch: 2499 Accuracy: 0.42: |ââââââââââââââââââââââââ--------------------------| 50.0% N of Epochs: 5000\r\n",
      "        Epoch: 2500\n",
      "        - Time:           2021-11-27 15:25:57.657753\n",
      "        - Train loss:     1.4903698859856143\n",
      "        - Validation loss 1.4873967478352208\n",
      "        - Accuracy        0.5031185031185031\n",
      "        \n",
      "Epoch: 2999 Accuracy: 0.5: |âââââââââââââââââââââââââââââ---------------------| 60.0% N of Epochs: 5000\r\n",
      "        Epoch: 3000\n",
      "        - Time:           2021-11-27 15:33:45.212303\n",
      "        - Train loss:     1.4517066213819716\n",
      "        - Validation loss 1.402478802588678\n",
      "        - Accuracy        0.5265072765072765\n",
      "        \n",
      "Epoch: 3499 Accuracy: 0.51: |ââââââââââââââââââââââââââââââââââ----------------| 70.0% N of Epochs: 5000\r\n",
      "        Epoch: 3500\n",
      "        - Time:           2021-11-27 15:40:52.856371\n",
      "        - Train loss:     1.4022144358060513\n",
      "        - Validation loss 1.5954296704261535\n",
      "        - Accuracy        0.47401247401247404\n",
      "        \n",
      "Epoch: 3999 Accuracy: 0.54: |âââââââââââââââââââââââââââââââââââââââ-----------| 80.0% N of Epochs: 5000\r\n",
      "        Epoch: 4000\n",
      "        - Time:           2021-11-27 15:48:54.739831\n",
      "        - Train loss:     1.371011511972773\n",
      "        - Validation loss 1.3755759308415074\n",
      "        - Accuracy        0.5369022869022869\n",
      "        \n",
      "Epoch: 4499 Accuracy: 0.58: |ââââââââââââââââââââââââââââââââââââââââââââ------| 90.0% N of Epochs: 5000\r\n",
      "        Epoch: 4500\n",
      "        - Time:           2021-11-27 15:56:51.073260\n",
      "        - Train loss:     1.336616281528919\n",
      "        - Validation loss 1.39844415649291\n",
      "        - Accuracy        0.5239085239085239\n",
      "        \n",
      "Epoch: 4989 Accuracy: 0.49: |âââââââââââââââââââââââââââââââââââââââââââââââââ-| 99.8% N of Epochs: 5000\r\n",
      "        Epoch: 4991\n",
      "        - Time:           2021-11-27 16:04:33.142217\n",
      "        - Train loss:     1.3199206583681162\n",
      "        - Validation loss 1.2992000426015546\n",
      "        - Accuracy        0.6096673596673596\n",
      "        \n",
      "\n",
      "        Epoch: 4992\n",
      "        - Time:           2021-11-27 16:04:34.157971\n",
      "        - Train loss:     1.326808496176848\n",
      "        - Validation loss 1.5282555011010939\n",
      "        - Accuracy        0.48492723492723494\n",
      "        \n",
      "\n",
      "        Epoch: 4993\n",
      "        - Time:           2021-11-27 16:04:35.168116\n",
      "        - Train loss:     1.3262506036730537\n",
      "        - Validation loss 1.2965927393205705\n",
      "        - Accuracy        0.5769230769230769\n",
      "        \n",
      "\n",
      "        Epoch: 4994\n",
      "        - Time:           2021-11-27 16:04:36.167031\n",
      "        - Train loss:     1.3331540443046748\n",
      "        - Validation loss 1.373074262372909\n",
      "        - Accuracy        0.5452182952182952\n",
      "        \n",
      "\n",
      "        Epoch: 4995\n",
      "        - Time:           2021-11-27 16:04:37.155119\n",
      "        - Train loss:     1.3385407436660857\n",
      "        - Validation loss 1.5002205102674422\n",
      "        - Accuracy        0.4896049896049896\n",
      "        \n",
      "\n",
      "        Epoch: 4996\n",
      "        - Time:           2021-11-27 16:04:38.130771\n",
      "        - Train loss:     1.339939576491975\n",
      "        - Validation loss 1.2981073606398799\n",
      "        - Accuracy        0.5867983367983368\n",
      "        \n",
      "\n",
      "        Epoch: 4997\n",
      "        - Time:           2021-11-27 16:04:39.120218\n",
      "        - Train loss:     1.3267236693560729\n",
      "        - Validation loss 1.409015328653397\n",
      "        - Accuracy        0.5135135135135135\n",
      "        \n",
      "\n",
      "        Epoch: 4998\n",
      "        - Time:           2021-11-27 16:04:40.145629\n",
      "        - Train loss:     1.3378638653030173\n",
      "        - Validation loss 1.5168872994761313\n",
      "        - Accuracy        0.4579002079002079\n",
      "        \n",
      "\n",
      "        Epoch: 4999\n",
      "        - Time:           2021-11-27 16:04:41.142766\n",
      "        - Train loss:     1.3531412144153439\n",
      "        - Validation loss 1.3125249685779694\n",
      "        - Accuracy        0.5602910602910602\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5000\n",
    "\n",
    "reducedLatinNet = ReducedLatinNet()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=reducedLatinNet.parameters(), lr=1e-3)\n",
    "\n",
    "reducedLatinNet.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_train = 0.0\n",
    "    loss_val = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for features, labels in reduced_train_loader:\n",
    "        outputs = reducedLatinNet(features)\n",
    "        train_loss = loss_fn(\n",
    "            outputs,\n",
    "            labels.long()\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += train_loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in reduced_val_loader:\n",
    "            outputs = reducedLatinNet(features)\n",
    "            val_loss = loss_fn(\n",
    "                outputs,\n",
    "                labels.long()\n",
    "            )\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "            loss_val += val_loss.item()\n",
    "\n",
    "    if \\\n",
    "        epoch % 500 == 0 \\\n",
    "        or epoch < 10 \\\n",
    "        or epoch > n_epochs - 10:\n",
    "        print(f'''\n",
    "        Epoch: {epoch}\n",
    "        - Time:           {datetime.datetime.now()}\n",
    "        - Train loss:     {loss_train/len(reduced_train_loader)}\n",
    "        - Validation loss {loss_val / len(reduced_val_loader)}\n",
    "        - Accuracy        {correct / total}\n",
    "        ''')\n",
    "    elif epoch % 500 != 0\\\n",
    "        and 10 < epoch < n_epochs - 10:\n",
    "        printProgressBar(\n",
    "            epoch,\n",
    "            n_epochs,\n",
    "            length=50,\n",
    "            prefix=f'Epoch: {epoch} Accuracy: {round(correct / total, 2)}:',\n",
    "            suffix=f'N of Epochs: {n_epochs}'\n",
    "        )\n",
    "\n",
    "    if round(correct / total, 2) >= 0.85:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "outputs": [],
   "source": [
    "from main import VersionFormatter\n",
    "\n",
    "version = VersionFormatter.by_datetime()\n",
    "REDUCEDLATINNET_PATH = 'D:/Documents/MAI/Course/models/reducedlatinnet/ReducedLatinNet_{version}'\n",
    "\n",
    "torch.save(reducedLatinNet, REDUCEDLATINNET_PATH.format(version = version))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "UMAP_PICKLE_PATH = 'D:/Documents/MAI/Course/models/umap/UMAP_11_26_00_24_51'\n",
    "with open(UMAP_PICKLE_PATH, 'rb') as pickle_file:\n",
    "    reducer = pickle.load(pickle_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([20, 3])"
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_reduced, labels = iter(torch.utils.data.DataLoader(reduced_val_set, batch_size=20, shuffle=True)).next()\n",
    "features_reduced.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction: Y\n",
      "Label: Y\n",
      "Correct: True\n",
      "    \n",
      "\n",
      "Prediction: P\n",
      "Label: P\n",
      "Correct: True\n",
      "    \n",
      "\n",
      "Prediction: P\n",
      "Label: R\n",
      "Correct: False\n",
      "    \n",
      "\n",
      "Prediction: I\n",
      "Label: M\n",
      "Correct: False\n",
      "    \n",
      "\n",
      "Prediction: M\n",
      "Label: M\n",
      "Correct: True\n",
      "    \n",
      "\n",
      "Prediction: E\n",
      "Label: E\n",
      "Correct: True\n",
      "    \n",
      "\n",
      "Prediction: P\n",
      "Label: T\n",
      "Correct: False\n",
      "    \n",
      "\n",
      "Prediction: L\n",
      "Label: L\n",
      "Correct: True\n",
      "    \n",
      "\n",
      "Prediction: P\n",
      "Label: P\n",
      "Correct: True\n",
      "    \n",
      "\n",
      "Prediction: J\n",
      "Label: J\n",
      "Correct: True\n",
      "    \n",
      "\n",
      "Prediction: B\n",
      "Label: C\n",
      "Correct: False\n",
      "    \n",
      "\n",
      "Prediction: B\n",
      "Label: B\n",
      "Correct: True\n",
      "    \n",
      "\n",
      "Prediction: Z\n",
      "Label: X\n",
      "Correct: False\n",
      "    \n",
      "\n",
      "Prediction: W\n",
      "Label: W\n",
      "Correct: True\n",
      "    \n",
      "\n",
      "Prediction: C\n",
      "Label: D\n",
      "Correct: False\n",
      "    \n",
      "\n",
      "Prediction: O\n",
      "Label: O\n",
      "Correct: True\n",
      "    \n",
      "\n",
      "Prediction: Z\n",
      "Label: U\n",
      "Correct: False\n",
      "    \n",
      "\n",
      "Prediction: C\n",
      "Label: C\n",
      "Correct: True\n",
      "    \n",
      "\n",
      "Prediction: F\n",
      "Label: F\n",
      "Correct: True\n",
      "    \n",
      "\n",
      "Prediction: W\n",
      "Label: W\n",
      "Correct: True\n",
      "    \n",
      "\n",
      "\n",
      "Summary\n",
      "- Accuracy: 0.65\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1800x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# get sample outputs\n",
    "output = reducedLatinNet(features_reduced)\n",
    "# convert output probabilities to predicted class\n",
    "_, preds = torch.max(output, 1)\n",
    "# prep images for display\n",
    "total = 0\n",
    "correct = 0\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    print(f'''\n",
    "Prediction: {string.ascii_uppercase[int(preds[idx].item())]}\n",
    "Label: {string.ascii_uppercase[int(labels[idx].item())]}\n",
    "Correct: {preds[idx] == labels[idx]}\n",
    "    ''')\n",
    "    total += 1\n",
    "    correct += 1 if preds[idx] == labels[idx] else 0\n",
    "    #display(Markdown(f'{string.ascii_uppercase[int(labels[idx].item())]} <span style=\"color: {\"green\" if preds[idx] == labels[idx] else \"red\"}\">{string.ascii_uppercase[int(preds[idx].item())]}</span>'))\n",
    "\n",
    "    # ax.set_title(\n",
    "    #     \"{} ({})\".format(string.ascii_uppercase[int(preds[idx].item())],,\n",
    "    #     color=(\"green\" if preds[idx] == labels[idx] else \"red\"))\n",
    "\n",
    "print(f'''\n",
    "\n",
    "Summary\n",
    "- Accuracy: {correct/total}\n",
    "''')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}