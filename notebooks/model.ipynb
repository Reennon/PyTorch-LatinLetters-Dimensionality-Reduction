{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import seaborn_image as isns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comnist_data = np.loadtxt('./../datasets/latin_data.csv', delimiter=\",\", dtype=\"float32\")\n",
    "comnist_label = np.loadtxt('./../datasets/latin_label.csv', delimiter=\",\", dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying letter G\n",
      "No. of Obs. : 784\n",
      "Min. Value : 0.0\n",
      "Max. Value : 1.0\n",
      "Mean : 0.08124786615371704\n",
      "Variance : 0.06019669398665428\n",
      "Skewness : 2.953777313232422\n"
     ]
    },
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "i = np.random.randint(comnist_data.shape[0])\n",
    "my_letter = np.flip(comnist_data[i].reshape(28, 28), 0)\n",
    "my_label = string.ascii_uppercase[int(comnist_label[i])]\n",
    "print(f\"Displaying letter {my_label}\")\n",
    "isns.imgplot(my_letter, cmap=\"gray\", describe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "\n",
    "\n",
    "class СoMNISTDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        labels_dir,\n",
    "        data_dir,\n",
    "        transform=None,\n",
    "        target_transform=None\n",
    "    ):\n",
    "        self.img_labels = pd.read_csv(labels_dir)\n",
    "        self.img_features = pd.read_csv(data_dir)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.asarray(self.img_features.iloc[idx], dtype=np.float32).reshape(28,28)\n",
    "        label = self.img_labels.iloc[idx, 0]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training_samples: 10897\n",
      "validation_samples: 1924\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'isns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-30-4de7a7ecc865>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     20\u001B[0m ''')\n\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m \u001B[0misns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomnnist_dataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m100\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     23\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomnnist_dataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m100\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'isns' is not defined"
     ]
    }
   ],
   "source": [
    "comnnist_dataset = СoMNISTDataset(\n",
    "    labels_dir='./../datasets/latin_label.csv',\n",
    "    data_dir='./../datasets/latin_data.csv',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((64,64)),\n",
    "        transforms.functional.vflip,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=0.0942,\n",
    "                             std=0.2352)\n",
    "    ])\n",
    ")\n",
    "# TODO: Refactor to const file etc\n",
    "TRAINING_PERCENT = 0.85\n",
    "training_samples = int(len(comnnist_dataset) * TRAINING_PERCENT)\n",
    "validation_samples = len(comnnist_dataset) - training_samples\n",
    "print(f'''\n",
    "training_samples: {training_samples}\n",
    "validation_samples: {validation_samples}\n",
    "''')\n",
    "\n",
    "isns.imshow(comnnist_dataset[100][0].squeeze(0))\n",
    "print(comnnist_dataset[100][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image = comnnist_dataset[100][0]\n",
    "plt.imshow(image.permute(1, 2 ,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imgs = torch.stack([img_t for img_t, _ in comnnist_dataset], dim=3)\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imgs.view(1, -1).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imgs.view(1, -1).std(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(\n",
    "    comnnist_dataset,\n",
    "    [training_samples, validation_samples],\n",
    "    generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: unsure to refactor batch_size to be in separate variable\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_i = iter(train_loader)\n",
    "images, labels = data_i.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20 / 2, idx + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(string.ascii_uppercase[int(labels[idx].item())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "width, height = img.shape\n",
    "thresh = img.max() / 2.5\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        val = round(img[x][y], 2) if img[x][y] != 0 else 0\n",
    "        ax.annotate(str(val), xy=(y, x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    color='white' if img[x][y] < thresh else 'black')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_out=26):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 26, dropout: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img, _ = comnnist_dataset[0]\n",
    "img_batch = img.unsqueeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "isns.imshow(img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alexnet = AlexNet()\n",
    "out = alexnet(img.unsqueeze(0))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "out = model(img.unsqueeze(0))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Specify loss and optimization functions\n",
    "\n",
    "# specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.Adam(alexnet.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "alexnet.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_train = 0.0\n",
    "    loss_val = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        outputs = alexnet(images)\n",
    "        train_loss = loss_fn(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += train_loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = alexnet(images)\n",
    "            val_loss = loss_fn(outputs, labels)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "            loss_val += val_loss.item()\n",
    "\n",
    "    \n",
    "    print(f'{datetime.datetime.now()} Epoch {epoch}, Training loss {loss_train / len(train_loader)}' +\n",
    "          f', Validation loss {loss_val / len(val_loader)}, Accuracy {correct / total}')\n",
    "    \n",
    "    if loss_val / len(val_loader) <= 0.3 or loss_train / len(train_loader) <=0.15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(dict(epoch=epoch_list, loss=train_loss_list))\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "fig = sns.relplot(\n",
    "    x='epoch'\n",
    "    , y='loss'\n",
    "    , kind='scatter'\n",
    "    , data=data\n",
    ")\n",
    "fig.savefig(\"output_3_80_relu.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "# obtain one batch of test images\n",
    "loader = torch.utils.data.DataLoader(val_set, batch_size=20, shuffle=True)\n",
    "dataiter = iter(loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# get sample outputs\n",
    "output = alexnet(images)\n",
    "# convert output probabilities to predicted class\n",
    "_, preds = torch.max(output, 1)\n",
    "# prep images for display\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20 / 2, idx + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(\n",
    "        \"{} ({})\".format(string.ascii_uppercase[int(preds[idx].item())],string.ascii_uppercase[int(labels[idx].item())]),\n",
    "        color=(\"green\" if preds[idx] == labels[idx] else \"red\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# torch.zeros(1).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval()  # prep model for *evaluation*\n",
    "\n",
    "for data, target in test_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss\n",
    "    test_loss += loss.item() * data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(100):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)')\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dimensionality Reduction using Supervised UMAP\n",
    "Using UMAP implementation from sci-kit learn library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from umap import UMAP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "reducer = UMAP(\n",
    "    n_neighbors=100,\n",
    "    n_components=3,\n",
    "    n_epochs=1000,\n",
    "    min_dist=0.5,\n",
    "    local_connectivity=10,\n",
    "    random_state=42\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_res: (10897, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train_res = 0\n",
    "X_test_res = 0\n",
    "batch = 0\n",
    "\n",
    "reducer_train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=True)\n",
    "reducer_val_loader = torch.utils.data.DataLoader(val_set, batch_size=len(val_set), shuffle=False)\n",
    "\n",
    "X_train_res = np.array()\n",
    "X_test_res = np.array()\n",
    "\n",
    "for images, labels in reducer_train_loader:\n",
    "    # TODO: Notate batch size trick\n",
    "    batch_size = images.shape[0]\n",
    "    images_a = images.reshape(batch_size,-1).numpy()\n",
    "    labels_a = labels.numpy()\n",
    "    X_train_res = reducer.fit_transform(images_a, labels_a)\n",
    "\n",
    "    break\n",
    "\n",
    "\n",
    "print(f'''Shape of X_train_res: {X_train_res.shape}''')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def chart(X, y):\n",
    "    #--------------------------------------------------------------------------#\n",
    "    # This section is not mandatory as its purpose is to sort the data by label\n",
    "    # so, we can maintain consistent colors for digits across multiple graphs\n",
    "\n",
    "    # Concatenate X and y arrays\n",
    "    arr_concat=np.concatenate((X, y.reshape(y.shape[0],1)), axis=1)\n",
    "    # Create a Pandas dataframe using the above array\n",
    "    df=pd.DataFrame(arr_concat, columns=['x', 'y', 'z', 'label'])\n",
    "    # Convert label data type from float to integer\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    # Finally, sort the dataframe by label\n",
    "    df.sort_values(by='label', axis=0, ascending=True, inplace=True)\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    # Create a 3D graph\n",
    "    fig = px.scatter_3d(df, x='x', y='y', z='z', color=df['label'].astype(str), height=900, width=950)\n",
    "\n",
    "    # Update chart looks\n",
    "    fig.update_layout(title_text='UMAP',\n",
    "                      showlegend=True,\n",
    "                      legend=dict(orientation=\"h\", yanchor=\"top\", y=0, xanchor=\"center\", x=0.5),\n",
    "                      scene_camera=dict(up=dict(x=0, y=0, z=1),\n",
    "                                            center=dict(x=0, y=0, z=-0.1),\n",
    "                                            eye=dict(x=1.5, y=-1.4, z=0.5)),\n",
    "                                            margin=dict(l=0, r=0, b=0, t=0),\n",
    "                      scene = dict(xaxis=dict(backgroundcolor='white',\n",
    "                                              color='black',\n",
    "                                              gridcolor='#f0f0f0',\n",
    "                                              title_font=dict(size=10),\n",
    "                                              tickfont=dict(size=10),\n",
    "                                             ),\n",
    "                                   yaxis=dict(backgroundcolor='white',\n",
    "                                              color='black',\n",
    "                                              gridcolor='#f0f0f0',\n",
    "                                              title_font=dict(size=10),\n",
    "                                              tickfont=dict(size=10),\n",
    "                                              ),\n",
    "                                   zaxis=dict(backgroundcolor='lightgrey',\n",
    "                                              color='black',\n",
    "                                              gridcolor='#f0f0f0',\n",
    "                                              title_font=dict(size=10),\n",
    "                                              tickfont=dict(size=10),\n",
    "                                             )))\n",
    "    # Update marker size\n",
    "    fig.update_traces(marker=dict(size=3, line=dict(color='black', width=0.1)))\n",
    "\n",
    "    fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for images, labels in reducer_val_loader:\n",
    "    batch_size = images.shape[0]\n",
    "    images_a = images.reshape(batch_size, -1).numpy()\n",
    "    X_test_res = reducer.transform(images_a)\n",
    "    chart(X_test_res, labels)\n",
    "\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Artificial Neural Network Model\n",
    "That can distinct our reduced dataset by UMAP, from 3 input features to 26 classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class ReducedLatinNet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 26, dropout: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3, 52)\n",
    "        self.fc1_dropout = nn.Dropout(p=dropout)\n",
    "        self.fc2 = nn.Linear(52, 52)\n",
    "        self.fc2_dropout = nn.Dropout(p=dropout)\n",
    "        self.fc3 = nn.Linear(52, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(torch.tanh(x))\n",
    "        out = self.fc1_dropout(out)\n",
    "        out = self.fc2(torch.tanh(out))\n",
    "        out = self.fc2_dropout(out)\n",
    "        out = self.fc3(torch.tanh(out))\n",
    "        return out\n",
    "\n",
    "reducedLatinNet = ReducedLatinNet()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "ReducedLatinNet(\n  (fc1): Linear(in_features=3, out_features=52, bias=True)\n  (fc1_dropout): Dropout(p=0.5, inplace=False)\n  (fc2): Linear(in_features=52, out_features=52, bias=True)\n  (fc2_dropout): Dropout(p=0.5, inplace=False)\n  (fc3): Linear(in_features=52, out_features=26, bias=True)\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducedLatinNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(12822, 784)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comnist_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([12822, 3]), (12822,))"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_images_a = np.load('./../datasets/reduced/latin_data.npy')\n",
    "reduced_images = torch.as_tensor(reduced_images_a)\n",
    "reduced_images.shape, comnist_label.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "reducedCoMNISTDataset = TensorDataset(reduced_images, torch.as_tensor(comnist_label))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "TRAINING_PERCENT = 0.85\n",
    "training_samples = int(len(reducedCoMNISTDataset) * TRAINING_PERCENT)\n",
    "validation_samples = len(reducedCoMNISTDataset) - training_samples\n",
    "\n",
    "reduced_train_set, reduced_val_set = torch.utils.data.random_split(\n",
    "    reducedCoMNISTDataset,\n",
    "    [training_samples, validation_samples],\n",
    "    generator=torch.Generator().manual_seed(42))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "reduced_train_loader = torch.utils.data.DataLoader(reduced_train_set, batch_size=1, shuffle=True)\n",
    "reduced_val_loader = torch.utils.data.DataLoader(reduced_val_set, batch_size=1, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([26])) must be the same as input size (torch.Size([26, 26]))",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-43-da1c0cfb8628>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mfeatures\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mreduced_train_loader\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m         \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mreducedLatinNet\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m         \u001B[0mtrain_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     17\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m         \u001B[0mtrain_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\python\\minicondaenv\\main\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\python\\minicondaenv\\main\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    712\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    713\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpos_weight\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpos_weight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 714\u001B[1;33m         return F.binary_cross_entropy_with_logits(input, target,\n\u001B[0m\u001B[0;32m    715\u001B[0m                                                   \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    716\u001B[0m                                                   \u001B[0mpos_weight\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpos_weight\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\python\\minicondaenv\\main\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mbinary_cross_entropy_with_logits\u001B[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001B[0m\n\u001B[0;32m   2825\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2826\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtarget\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2827\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Target size ({}) must be the same as input size ({})\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtarget\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2828\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2829\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbinary_cross_entropy_with_logits\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpos_weight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduction_enum\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Target size (torch.Size([26])) must be the same as input size (torch.Size([26, 26]))"
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "\n",
    "reducedLatinNet.train()\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(params=reducedLatinNet.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_train = 0.0\n",
    "    loss_val = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for features, labels in reduced_train_loader:\n",
    "        outputs = reducedLatinNet(features)\n",
    "        train_loss = loss_fn(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += train_loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in reduced_val_loader:\n",
    "            outputs = reducedLatinNet(features)\n",
    "            val_loss = loss_fn(outputs, labels)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "            loss_val += val_loss.item()\n",
    "\n",
    "\n",
    "    print(f'{datetime.datetime.now()} Epoch {epoch}, Training loss {loss_train / len(train_loader)}' +\n",
    "          f', Validation loss {loss_val / len(val_loader)}, Accuracy {correct / total}')\n",
    "\n",
    "    if loss_val / len(val_loader) <= 0.3 or loss_train / len(train_loader) <=0.15:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}